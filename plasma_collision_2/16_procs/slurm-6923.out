[n002:32504] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32504] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32512] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32512] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32500] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32500] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32506] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32506] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32511] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32511] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32502] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32502] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32503] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32503] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32507] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32507] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32505] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32505] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32510] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32510] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32509] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32509] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32497] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32497] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32501] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32501] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32499] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32499] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32508] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32508] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[n002:32498] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[n002:32498] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              n002
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n002
  Local device: mlx5_0
--------------------------------------------------------------------------
                    _            _
  ___           _  | |        _  \ \   Version : 4.6-237-g21b2d0b-master
 / __|  _ __   (_) | |  ___  (_)  | |   
 \__ \ | '  \   _  | | / -_)  _   | |
 |___/ |_|_|_| |_| |_| \___| |_|  | |  
                                 /_/    
 
 

 Reading the simulation parameters
 --------------------------------------------------------------------------------
 HDF5 version 1.12.1
	 Python version 3.6.8
	 Parsing pyinit.py
	 Parsing 4.6-237-g21b2d0b-master
	 Parsing pyprofiles.py
	 Parsing /home/reynaldo.rojas/smilei/Smilei/Smilei-benchmarks/plasma_collision_2/16_procs/plasma_collision.py
	 Parsing pycontrol.py
	 Check for function preprocess()
	 python preprocess function does not exist
	 Calling python _smilei_check
	 Calling python _prepare_checkpoint_dir
	 Calling python _keep_python_running() :
	[WARNING] Patches distribution: hilbertian
 

 Geometry: 2Dcartesian
 --------------------------------------------------------------------------------
	 Interpolation order : 2
	 Maxwell solver : Yee
	 simulation duration = 188.495559,   total number of iterations = 2250
	 timestep = 0.083776 = 0.942809 x CFL,   time resolution = 11.936621
	 Grid length: 251.327, 31.4159
	 Cell length: 0.125664, 0.125664, 0
	 Number of cells: 2000, 250
	 Spatial resolution: 7.95775, 7.95775
 

 Electromagnetic boundary conditions
 --------------------------------------------------------------------------------
	 xmin silver-muller, absorbing vector [1, 0]
	 xmax silver-muller, absorbing vector [-1, -0]
	 ymin periodic
	 ymax periodic
 

 Vectorization: 
 --------------------------------------------------------------------------------
	 Mode: off
 

 Initializing MPI
 --------------------------------------------------------------------------------
	 applied topology for periodic BCs in y-direction
	 MPI_THREAD_MULTIPLE enabled
	 Number of MPI processes: 16
	 Number of threads per MPI process : 1
 
	 Number of patches: 16 x 2
	 Number of cells in one patch: 125 x 125
	 Dynamic load balancing: never
 

 Initializing the restart environment
 --------------------------------------------------------------------------------
 
 

 Initializing species
 --------------------------------------------------------------------------------
	 
	 Creating Species #0: pon1
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 0.000000, yvacuum: 0.000000, xplateau: 62.831853, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #1: eon1
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 0.000000, yvacuum: 0.000000, xplateau: 62.831853, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #2: pon2
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 69.115038, yvacuum: 0.000000, xplateau: 201.061930, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #3: eon2
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 69.115038, yvacuum: 0.000000, xplateau: 201.061930, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
 

 Initializing Patches
 --------------------------------------------------------------------------------
	 First patch created
		 Approximately 10% of patches created
	 All patches created
 

 Creating Diagnostics, antennas, and external fields
 --------------------------------------------------------------------------------
	 Created performances diagnostic
 

 finalize MPI
 --------------------------------------------------------------------------------
	 Done creating diagnostics, antennas, and external fields
 

 Minimum memory consumption (does not include all temporary buffers)
 --------------------------------------------------------------------------------
              Particles: Master 17 MB;   Max 17 MB;   Global 0.272 GB
                 Fields: Master 3 MB;   Max 3 MB;   Global 0.0524 GB
            scalars.txt: Master 0 MB;   Max 0 MB;   Global 0 GB
        Performances.h5: Master 0 MB;   Max 0 MB;   Global 0 GB
 

 Initial fields setup
 --------------------------------------------------------------------------------
	 Solving Poisson at time t = 0
 

 Initializing E field through Poisson solver
 --------------------------------------------------------------------------------
	 Poisson solver converged at iteration: 3067, relative err is ctrl = 0.968869 x 1e-14
	 Poisson equation solved. Maximum err = 0.000000 at i= -1
 Time in Poisson : 0.785392
	 Applying external fields at time t = 0
	 Applying prescribed fields at time t = 0
	 Applying antennas at time t = 0
 

 Open files & initialize diagnostics
 --------------------------------------------------------------------------------
 

 Running diags at time t = 0
 --------------------------------------------------------------------------------
 

 Species creation summary
 --------------------------------------------------------------------------------
		 Species 0 (pon1) created with 750000 particles
		 Species 1 (eon1) created with 750000 particles
		 Species 2 (pon2) created with 2175000 particles
		 Species 3 (eon2) created with 2175000 particles
 

 Expected disk usage (approximate)
 --------------------------------------------------------------------------------
	 WARNING: disk usage by non-uniform particles maybe strongly underestimated,
	    especially when particles are created at runtime (ionization, pair generation, etc.)
	 
	 Expected disk usage for diagnostics:
		 File Performances.h5: 257.37 K
		 File scalars.txt: 0 bytes
	 Total disk usage for diagnostics: 257.37 K
	 
 

 Keeping or closing the python runtime environment
 --------------------------------------------------------------------------------
	 Checking for cleanup() function:
	 python cleanup function does not exist
	 Closing Python
 

 Time-Loop started: number of time-steps n_time = 2250
 --------------------------------------------------------------------------------
	[WARNING] The following `push time` assumes a global number of 16 cores (hyperthreading is unknown)
    timestep       sim time   cpu time [s]   (    diff [s] )   push time [ns]
    225/2250     1.8891e+01     2.7735e+01   (  2.7735e+01 )             337 
    450/2250     3.7741e+01     6.9288e+01   (  4.1553e+01 )             505 
    675/2250     5.6591e+01     1.2102e+02   (  5.1735e+01 )             628 
    900/2250     7.5440e+01     1.8080e+02   (  5.9782e+01 )             726 
   1125/2250     9.4290e+01     2.4509e+02   (  6.4285e+01 )             781 
   1350/2250     1.1314e+02     3.1045e+02   (  6.5360e+01 )             794 
   1575/2250     1.3199e+02     3.7673e+02   (  6.6278e+01 )             805 
   1800/2250     1.5084e+02     4.4004e+02   (  6.3308e+01 )             769 
   2025/2250     1.6969e+02     5.0024e+02   (  6.0200e+01 )             731 
   2250/2250     1.8854e+02     5.5891e+02   (  5.8676e+01 )             713 
 

 End time loop, time dual = 1.8854e+02
 --------------------------------------------------------------------------------
 

 Time profiling : (print time > 0.001%)
 --------------------------------------------------------------------------------
 Time_in_time_loop	5.5891e+02	9.9770e+01% coverage
 	           Particles	1.809487e+02	3.2e+01%
 	             Maxwell	1.138915e+00	    <1%
 	         Diagnostics	8.690819e+00	1.6e+00%
 	      Sync Particles	1.870646e+02	3.3e+01%
 	         Sync Fields	4.062176e+01	7.3e+00%
 	      Sync Densities	1.397456e+02	2.5e+01%
 
	 Printed times are averaged per MPI process
 		 See advanced metrics in profil.txt
 
	Diagnostics profile :
 		         scalars.txt	2.2e-02
 		     Performances.h5	8.7e+00
 

 END
 --------------------------------------------------------------------------------

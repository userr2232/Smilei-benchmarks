[g001:38306] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38318] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38318] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38307] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38307] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38309] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38309] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38315] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38315] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38311] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38311] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38314] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38314] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38310] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38310] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38317] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38317] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38312] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38312] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38313] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38313] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38316] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38316] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38319] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38319] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38308] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38308] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38320] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38320] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38305] PMIX ERROR: BAD-PARAM in file base/bfrop_base_unpack.c at line 692
[g001:38305] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
[g001:38306] PMIX ERROR: BAD-PARAM in file dstore_base.c at line 2225
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              g001
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   g001
  Local device: mlx5_0
--------------------------------------------------------------------------
                    _            _
  ___           _  | |        _  \ \   Version : 4.6-237-g21b2d0b-master
 / __|  _ __   (_) | |  ___  (_)  | |   
 \__ \ | '  \   _  | | / -_)  _   | |
 |___/ |_|_|_| |_| |_| \___| |_|  | |  
                                 /_/    
 
 

 Reading the simulation parameters
 --------------------------------------------------------------------------------
 HDF5 version 1.12.1
	 Python version 3.6.8
	 Parsing pyinit.py
	 Parsing 4.6-237-g21b2d0b-master
	 Parsing pyprofiles.py
	 Parsing /home/reynaldo.rojas/smilei/Smilei/Smilei-benchmarks/plasma_collision_4/16_procs/plasma_collision.py
	 Parsing pycontrol.py
	 Check for function preprocess()
	 python preprocess function does not exist
	 Calling python _smilei_check
	 Calling python _prepare_checkpoint_dir
	 Calling python _keep_python_running() :
	[WARNING] Patches distribution: hilbertian
 

 Geometry: 2Dcartesian
 --------------------------------------------------------------------------------
	 Interpolation order : 2
	 Maxwell solver : Yee
	 simulation duration = 188.495559,   total number of iterations = 2250
	 timestep = 0.083776 = 0.942809 x CFL,   time resolution = 11.936621
	 Grid length: 251.327, 31.4159
	 Cell length: 0.125664, 0.125664, 0
	 Number of cells: 2000, 250
	 Spatial resolution: 7.95775, 7.95775
 

 Electromagnetic boundary conditions
 --------------------------------------------------------------------------------
	 xmin silver-muller, absorbing vector [1, 0]
	 xmax silver-muller, absorbing vector [-1, -0]
	 ymin periodic
	 ymax periodic
 

 Vectorization: 
 --------------------------------------------------------------------------------
	 Mode: off
 

 Initializing MPI
 --------------------------------------------------------------------------------
	 applied topology for periodic BCs in y-direction
	 MPI_THREAD_MULTIPLE enabled
	 Number of MPI processes: 16
	 Number of threads per MPI process : 1
 
	 Number of patches: 16 x 2
	 Number of cells in one patch: 125 x 125
	 Dynamic load balancing: never
 

 Initializing the restart environment
 --------------------------------------------------------------------------------
 
 

 Initializing species
 --------------------------------------------------------------------------------
	 
	 Creating Species #0: pon1
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 0.000000, yvacuum: 0.000000, xplateau: 62.831853, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #1: eon1
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 0.000000, yvacuum: 0.000000, xplateau: 62.831853, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #2: pon2
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 69.115038, yvacuum: 0.000000, xplateau: 201.061930, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
	 
	 Creating Species #3: eon2
		 > Pusher: boris
		 > Density profile: 2D built-in profile `trapezoidal` (value: 1.000000, xvacuum: 69.115038, yvacuum: 0.000000, xplateau: 201.061930, yplateau: 31.415927, xslope1: 0.000000, xslope2: 0.000000, yslope1: 0.000000, yslope2: 0.000000)
 

 Initializing Patches
 --------------------------------------------------------------------------------
	 First patch created
		 Approximately 10% of patches created
	 All patches created
 

 Creating Diagnostics, antennas, and external fields
 --------------------------------------------------------------------------------
	 Created performances diagnostic
 

 finalize MPI
 --------------------------------------------------------------------------------
	 Done creating diagnostics, antennas, and external fields
 

 Minimum memory consumption (does not include all temporary buffers)
 --------------------------------------------------------------------------------
              Particles: Master 29 MB;   Max 29 MB;   Global 0.454 GB
                 Fields: Master 3 MB;   Max 3 MB;   Global 0.0524 GB
            scalars.txt: Master 0 MB;   Max 0 MB;   Global 0 GB
        Performances.h5: Master 0 MB;   Max 0 MB;   Global 0 GB
 

 Initial fields setup
 --------------------------------------------------------------------------------
	 Solving Poisson at time t = 0
 

 Initializing E field through Poisson solver
 --------------------------------------------------------------------------------
	 Poisson solver converged at iteration: 3042, relative err is ctrl = 0.991245 x 1e-14
	 Poisson equation solved. Maximum err = 0.000000 at i= -1
 Time in Poisson : 0.791758
	 Applying external fields at time t = 0
	 Applying prescribed fields at time t = 0
	 Applying antennas at time t = 0
 

 Open files & initialize diagnostics
 --------------------------------------------------------------------------------
 

 Running diags at time t = 0
 --------------------------------------------------------------------------------
 

 Species creation summary
 --------------------------------------------------------------------------------
		 Species 0 (pon1) created with 1250000 particles
		 Species 1 (eon1) created with 1250000 particles
		 Species 2 (pon2) created with 3625000 particles
		 Species 3 (eon2) created with 3625000 particles
 

 Expected disk usage (approximate)
 --------------------------------------------------------------------------------
	 WARNING: disk usage by non-uniform particles maybe strongly underestimated,
	    especially when particles are created at runtime (ionization, pair generation, etc.)
	 
	 Expected disk usage for diagnostics:
		 File Performances.h5: 257.37 K
		 File scalars.txt: 0 bytes
	 Total disk usage for diagnostics: 257.37 K
	 
 

 Keeping or closing the python runtime environment
 --------------------------------------------------------------------------------
	 Checking for cleanup() function:
	 python cleanup function does not exist
	 Closing Python
 

 Time-Loop started: number of time-steps n_time = 2250
 --------------------------------------------------------------------------------
	[WARNING] The following `push time` assumes a global number of 16 cores (hyperthreading is unknown)
    timestep       sim time   cpu time [s]   (    diff [s] )   push time [ns]
    225/2250     1.8891e+01     4.8280e+01   (  4.8280e+01 )             352 
    450/2250     3.7741e+01     1.1867e+02   (  7.0387e+01 )             513 
    675/2250     5.6591e+01     2.0640e+02   (  8.7729e+01 )             639 
    900/2250     7.5440e+01     3.0710e+02   (  1.0070e+02 )             734 
   1125/2250     9.4290e+01     4.1574e+02   (  1.0864e+02 )             792 
   1350/2250     1.1314e+02     5.2780e+02   (  1.1206e+02 )             817 
   1575/2250     1.3199e+02     6.4101e+02   (  1.1321e+02 )             825 
   1800/2250     1.5084e+02     7.4856e+02   (  1.0755e+02 )             784 
   2025/2250     1.6969e+02     8.5363e+02   (  1.0508e+02 )             766 
   2250/2250     1.8854e+02     9.5595e+02   (  1.0232e+02 )             746 
 

 End time loop, time dual = 1.8854e+02
 --------------------------------------------------------------------------------
 

 Time profiling : (print time > 0.001%)
 --------------------------------------------------------------------------------
 Time_in_time_loop	9.5596e+02	9.9800e+01% coverage
 	           Particles	3.048195e+02	3.2e+01%
 	             Maxwell	1.288525e+00	    <1%
 	         Diagnostics	1.366737e+01	1.4e+00%
 	      Sync Particles	3.283163e+02	3.4e+01%
 	         Sync Fields	6.814186e+01	7.1e+00%
 	      Sync Densities	2.387984e+02	2.5e+01%
 
	 Printed times are averaged per MPI process
 		 See advanced metrics in profil.txt
 
	Diagnostics profile :
 		         scalars.txt	1.8e-02
 		     Performances.h5	1.4e+01
 

 END
 --------------------------------------------------------------------------------
